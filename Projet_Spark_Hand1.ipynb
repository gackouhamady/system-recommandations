{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Projet : Syst√®me de Recommandation en Temps R√©el pour une Plateforme de Streaming avec Apache Spark et Hadoop**\n",
        "\n",
        "## **Auteur :** Hamady GACKOU"
      ],
      "metadata": {
        "id": "VVGJKchIMWc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contexte** :\n",
        "\n",
        "## Objectif :\n",
        "Cr√©er un pipeline de traitement de donn√©es distribu√©es pour une plateforme de streaming (films, s√©ries, ou musique) en utilisant Hadoop et Apache Spark. L'objectif est d'exploiter des donn√©es massives (logs des utilisateurs, m√©tadonn√©es des contenus, interactions) pour g√©n√©rer des recommandations en temps r√©el tout en optimisant le stockage et le traitement des donn√©es.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Hcp9_kgMth1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Architecture du Projet**\n",
        "### **Stockage des Donn√©es :**\n",
        "- HDFS : Stockage distribu√© des donn√©es des utilisateurs et des contenus.\n",
        "- Parquet / Avro : Utilisation de formats de stockage efficaces pour les donn√©es structur√©es et semi-structur√©es.\n",
        "- Hive / HBase : Base de donn√©es pour g√©rer l'acc√®s aux informations des utilisateurs et des contenus.\n",
        "- Kafka : Streaming en temps r√©el des √©v√©nements utilisateur (clics, lectures, ajouts aux favoris).\n",
        "\n",
        "### **Traitement des Donn√©es**\n",
        "- Apache Spark (RDD, DataFrame, Spark SQL) :\n",
        "- Nettoyage et transformation des donn√©es en RDD et DataFrames.\n",
        "- Utilisation de Spark SQL pour requ√™ter les donn√©es.\n",
        "- Spark Streaming : Traitement en temps r√©el des logs utilisateur pour mise √† jour dynamique des recommandations.\n",
        "- Machine Learning avec Spark MLlib : Entra√Ænement d'un mod√®le de recommandation bas√© sur ALS (Alternating Least Squares).\n",
        "- Orchestration et Workflow\n",
        "- Apache Airflow / Oozie : Gestion des workflows de traitement batch et en streaming.\n",
        "## **2. D√©tails de Mise en ≈íuvre**\n",
        "### **Phase 1 : Ingestion et Stockage des Donn√©es**\n",
        "- Collecte des donn√©es\n",
        "\n",
        "- Importation de logs utilisateur via Kafka.\n",
        "- Importation des m√©tadonn√©es des films via Sqoop (depuis une base relationnelle vers HDFS).\n",
        "Stockage structur√©\n",
        "\n",
        "- Sauvegarde des donn√©es utilisateur en HBase.\n",
        "- Sauvegarde des m√©tadonn√©es en Hive sous format Parquet.\n",
        "### **Phase 2 : Traitement et Analyse des Donn√©es**\n",
        "- Nettoyage et Pr√©paration des Donn√©es\n",
        "\n",
        "- Utilisation de Spark RDD pour filtrer et normaliser les donn√©es utilisateur.\n",
        "- Stockage des r√©sultats interm√©diaires sous Parquet.\n",
        "- Analyse et Requ√™tage\n",
        "\n",
        "- Cr√©ation de vues SQL avec Spark SQL pour comprendre les comportements des utilisateurs.\n",
        "- Identification des tendances et des contenus les plus populaires.\n",
        "- Recommandation avec Spark MLlib\n",
        "\n",
        "- Entra√Ænement d‚Äôun mod√®le ALS (Collaborative Filtering) sur les interactions utilisateur.\n",
        "- Optimisation des hyperparam√®tres et √©valuation du mod√®le.\n",
        "## **Phase 3 : Recommandations en Temps R√©el**\n",
        "- Traitement en Streaming avec Spark Streaming\n",
        "\n",
        "- Capture des √©v√©nements utilisateur (clics, visionnage) en temps r√©el via Kafka.\n",
        "- Mise √† jour dynamique du mod√®le de recommandation.\n",
        "- Mise en Production du Mod√®le\n",
        "\n",
        "- Stockage des recommandations mises √† jour en HBase.\n",
        "- API pour r√©cup√©rer les recommandations utilisateur.\n",
        "## **Phase 4 : Automatisation et Monitoring**\n",
        "- Orchestration avec Apache Airflow / Oozie\n",
        "\n",
        "- Planification des t√¢ches batch (mise √† jour des recommandations).\n",
        "- Automatisation de l'entra√Ænement du mod√®le tous les jours.\n",
        "- Monitoring avec Spark UI et Prometheus\n",
        "\n",
        "- Surveillance de la performance des jobs Spark.\n",
        "- Optimisation des ressources sur YARN / Mesos."
      ],
      "metadata": {
        "id": "8ZGi_1cONeb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå **√âtape 1 : Configuration de l‚Äôenvironnement**\n",
        "- Installez Java 8 et Apache Spark.\n",
        "- Configurez les variables d'environnement n√©cessaires.\n",
        "- Initialisez `findspark`."
      ],
      "metadata": {
        "id": "TTOFoTu7QYl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installer java  21,  Spark et ses d√©pendances\n",
        "!apt-get install openjdk-21-jdk-headless -qq > /dev/null\n",
        "! wget -q https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "! tar xf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# Configuration des paths\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "\n",
        "# Installer  findSpark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "rzSBcHH5QdEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RecommenderSystem\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Apache Spark est pr√™t ! üöÄ\")\n"
      ],
      "metadata": {
        "id": "eooGpLeaR-wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå √âtape 2 : Ingestion et Stockage des Donn√©es**"
      ],
      "metadata": {
        "id": "kdOqNX4LSMsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette √©tape consiste √† collecter les donn√©es utilisateur et les m√©tadonn√©es des films, puis √† les stocker dans HDFS et Hive."
      ],
      "metadata": {
        "id": "7YuL9CzoSUUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 T√©l√©charger un Jeu de Donn√©es (MovieLens)**"
      ],
      "metadata": {
        "id": "jtL9EbTiSbpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip -o ml-latest-small.zip"
      ],
      "metadata": {
        "id": "eFFRqVDISflm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Charger les Donn√©es avec PySpark**\n",
        "\n",
        "Cr√©ons un DataFrame PySpark pour charger les donn√©es :"
      ],
      "metadata": {
        "id": "fyMQERc6UVvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = spark.read.csv(\"ml-latest-small/ratings.csv\", header=True, inferSchema=True)\n",
        "movies = spark.read.csv(\"ml-latest-small/movies.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "ratings.show(5)\n",
        "movies.show(5)"
      ],
      "metadata": {
        "id": "lZyVlIDSUb6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìå √âtape 3 : Nettoyage et Pr√©paration des Donn√©es**"
      ],
      "metadata": {
        "id": "JGV55RgOUzH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous devons transformer et nettoyer les donn√©es pour les utiliser efficacement avec Spark MLlib.\n",
        "\n",
        "**3.1 Supprimer les Valeurs Nulles et Filtrer les Donn√©es**"
      ],
      "metadata": {
        "id": "wJeVl1BrVGku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings.dropna()\n",
        "movies = movies.dropna()\n",
        "\n",
        "# V√©rifier les valeurs uniques\n",
        "print(f\"Nombre de films uniques : {movies.select('movieId').distinct().count()}\")\n",
        "print(f\"Nombre d'utilisateurs uniques : {ratings.select('userId').distinct().count()}\")\n"
      ],
      "metadata": {
        "id": "83xMbeSzU8ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Conversion des Donn√©es pour Spark MLlib**\n",
        "\n",
        "Les algorithmes de recommandation utilisent des identifiants num√©riques. On va convertir les colonnes userId et movieId en integer."
      ],
      "metadata": {
        "id": "spjq7F08VXT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer_user = StringIndexer(inputCol=\"userId\", outputCol=\"userIndex\")\n",
        "indexer_movie = StringIndexer(inputCol=\"movieId\", outputCol=\"movieIndex\")\n",
        "\n",
        "ratings = indexer_user.fit(ratings).transform(ratings)\n",
        "ratings = indexer_movie.fit(ratings).transform(ratings)\n",
        "\n",
        "ratings.show(5)\n"
      ],
      "metadata": {
        "id": "Ci4B0VU_Vf2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìå √âtape 4 : Construction du Mod√®le de Recommandation**"
      ],
      "metadata": {
        "id": "2wwcLukHV6mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant entra√Æner un mod√®le de filtrage collaboratif en utilisant ALS (Alternating Least Squares) dans Spark MLlib."
      ],
      "metadata": {
        "id": "zpqFvC3iWChs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Configuration du mod√®le ALS\n",
        "als = ALS(\n",
        "    userCol=\"userIndex\",\n",
        "    itemCol=\"movieIndex\",\n",
        "    ratingCol=\"rating\",\n",
        "    nonnegative=True,\n",
        "    implicitPrefs=False,\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "C8KSI9PAV-S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Entra√Æner le Mod√®le**\n",
        "\n",
        "Nous allons diviser les donn√©es en ensemble d‚Äôentra√Ænement et de test (80% - 20%)."
      ],
      "metadata": {
        "id": "f6kEOk8OXETw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = ratings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "model = als.fit(train_data)\n",
        "\n",
        "# G√©n√©rer les pr√©dictions\n",
        "predictions = model.transform(test_data)\n",
        "predictions.show(5)"
      ],
      "metadata": {
        "id": "kW9zbMYiXLEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìå √âtape 5 : √âvaluation du Mod√®le**\n",
        "Nous utilisons la Root Mean Square Error (RMSE) pour mesurer la pr√©cision des recommandations."
      ],
      "metadata": {
        "id": "rmpwmkvfXsMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"RMSE du mod√®le : {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "n1lfSJjsXwa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìå √âtape 6 : Recommandations en Temps R√©el avec Spark Streaming**"
      ],
      "metadata": {
        "id": "dt6K6DQkYCbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant simuler des √©v√©nements utilisateur en temps r√©el avec Apache Kafka et Spark Streaming."
      ],
      "metadata": {
        "id": "DvdNcKgsYIjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.1 Installer Apache Kafk**"
      ],
      "metadata": {
        "id": "dGUITr_sZtGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "UpUjRqyFgtMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/kafka/3.7.2/kafka-3.7.2-src.tgz\n",
        "!tar -xzf kafka-3.7.2-src.tgz"
      ],
      "metadata": {
        "id": "3Kr9pVjQYF_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/kafka-3.7.2-src\n",
        "!chmod +x gradlew\n",
        "# !./gradlew jar -PscalaVersion=2.13.12\n",
        "!./gradlew  jar -PscalaVersion=2.13.12 -x test\n",
        "#!./gradlew --no-daemon jar -PscalaVersion=2.13.12 -x test"
      ],
      "metadata": {
        "id": "iRgtBNvfaQ-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gradlew clean releaseTarGz -PscalaVersion=2.13.12"
      ],
      "metadata": {
        "id": "eqH_ktf3kRgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf core/build/distributions/kafka_2.13-3.7.2.tgz"
      ],
      "metadata": {
        "id": "4nbGvo-Tl_No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2 D√©marrer un Producteur Kafka (Simulation d'√âv√©nements)**"
      ],
      "metadata": {
        "id": "9cuKgr_3ZwQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/kafka-3.7.2-src"
      ],
      "metadata": {
        "id": "osxF-Se_jOT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.7.2/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.7.2/config/zookeeper.properties\n",
        "!./kafka_2.13-3.7.2/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.7.2/config/server.properties\n"
      ],
      "metadata": {
        "id": "_7luR7vwknQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cr√©er un topic Kafka nomm√© user-events :***"
      ],
      "metadata": {
        "id": "5PnNUI-heYRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.7.2/bin/kafka-topics.sh --create --topic user-events --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"
      ],
      "metadata": {
        "id": "kqESYw4AaBJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìå Conclusion**\n",
        "Nous avons mis en place un syst√®me de recommandation bas√© sur Spark et Hadoop, incluant :\n",
        "- ‚úÖ Ingestion des donn√©es avec HDFS et Kafka\n",
        "- ‚úÖ Traitement avec Spark SQL et Spark MLlib\n",
        "- ‚úÖ G√©n√©ration des recommandations en temps r√©el avec Spark Streaming"
      ],
      "metadata": {
        "id": "YKcpEPsLiEu4"
      }
    }
  ]
}